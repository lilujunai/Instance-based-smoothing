{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Replace True Labels with Probabilities from KDE + Fitting LR\n",
    "#### y = Score_Positive / (Score_positive + Score_negative)\n",
    "\n",
    "\n",
    "## Method 2: Generate double points from KDE + LR fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR:\n",
    "    def __init__(self, learnRate = 0.001, nIter = 1000, use_intercept = True, smoothed = False):\n",
    "        self.learnRate = learnRate\n",
    "        self.nIter = nIter\n",
    "        self.intercept = use_intercept\n",
    "        self.smooth = smoothed\n",
    "        \n",
    "    def crossEntropy(self, P, Y):\n",
    "        return (-Y * np.log(P) - (1 - Y + 1e-9) * np.log(1 - P + 1e-9)).mean()\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def SGD(self, X_train, y_train, X_test, y_test, y_train_smoothed = np.zeros(1)):\n",
    "        if self.smooth == False:\n",
    "            y_train_smoothed = np.zeros(len(y_train))\n",
    "        #Add Intercept\n",
    "        if self.intercept == True:\n",
    "            X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)\n",
    "            X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "        #Initialize Weights by zeros\n",
    "        self.Ws = np.zeros(X_train.shape[1])\n",
    "        oldWs = np.zeros(X_train.shape[1])\n",
    "        CETEST = 1\n",
    "        self.converge = self.nIter\n",
    "        #Update weights for n Iterations\n",
    "        for i in range(self.nIter):\n",
    "            if i % 5000 == 0:\n",
    "                print('Finished ', i, ' iterations --> Test CE:', CETEST)\n",
    "            #Shuffle indeces\n",
    "            p = np.random.permutation(len(X_train))\n",
    "            X_train = X_train[p]\n",
    "            y_train = y_train[p]\n",
    "            y_train_smoothed = y_train_smoothed[p]\n",
    "            for row,y_t, y_t_s in zip(X_train, y_train, y_train_smoothed):\n",
    "                Z = np.dot(row, self.Ws)\n",
    "                y_p = self.sigmoid(Z)\n",
    "                if self.smooth:\n",
    "                    gradient = np.dot(row.T, (y_p - y_t_s))\n",
    "                else:\n",
    "                    gradient = np.dot(row.T, (y_p - y_t))\n",
    "                self.Ws -= self.learnRate * gradient\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                if (np.absolute(oldWs - self.Ws).sum() / len(self.Ws) ) <= 0.001:\n",
    "                    self.converge = i\n",
    "                    break\n",
    "                oldWs = self.Ws\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        #Add Intercept\n",
    "        if self.intercept == True:\n",
    "            intercept = np.ones((X.shape[0], 1))\n",
    "            X = np.concatenate((intercept, X), axis=1)\n",
    "        return self.sigmoid(np.dot(X, self.Ws))\n",
    "    \n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "dataset_names = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk('Datasets\\\\'):\n",
    "    for file in f:\n",
    "        if '.csv' in file:\n",
    "            dataset_names.append(file)\n",
    "            files.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'Name':[], 'Instances':[], 'Features':[], 'PosClassRatio':[], \n",
    "        'NTestCE':[], 'M1TestCE':[], 'M2TestCE':[], 'PTestCE':[], 'SKTestCE':[], \n",
    "        'NTrainCE':[], 'M1TrainCE':[], 'M2TrainCE':[], 'PTrainCE':[], 'SKTrainCE':[], \n",
    "        'NTestAcc':[], 'M1TestAcc':[], 'M2TestAcc':[], 'PTestAcc':[], 'SKTestAcc':[],\n",
    "        'NTrainAcc':[], 'M1TrainAcc':[], 'M2TrainAcc':[], 'PTrainAcc':[], 'SKTrainAcc':[],\n",
    "       'NConverge':[], 'M1Converge':[], 'M2Converge':[], 'PConverge':[]\n",
    "      }\n",
    "results = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRate = 0.001\n",
    "nIter = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_features(data):\n",
    "    n_rows = len(data)\n",
    "    n_feats = len(data.columns) - 1\n",
    "    pos_class_ratio = (data['class'] == 1).sum() / n_rows\n",
    "    return n_rows, n_feats, pos_class_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal LR Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_results(X_train, y_train, X_test, y_test):\n",
    "    model = LR(learnRate = LRate, nIter = nIter)\n",
    "    model.SGD(X_train.values, y_train.values, X_test.values, y_test.values)\n",
    "    \n",
    "    y_pred_test  = model.predict_prob(X_test)\n",
    "    y_pred_train = model.predict_prob(X_train)\n",
    "    \n",
    "    CE_test  = model.crossEntropy(y_pred_test,  y_test)\n",
    "    CE_train = model.crossEntropy(y_pred_train, y_train)\n",
    "    \n",
    "    y_pred_test_labels  = model.predict(X_test)\n",
    "    y_pred_train_labels = model.predict(X_train)\n",
    "    \n",
    "    acc_test  = accuracy_score(y_test,  y_pred_test_labels ) * 100\n",
    "    acc_train = accuracy_score(y_train, y_pred_train_labels) * 100\n",
    "    \n",
    "    return acc_test, CE_test, acc_train, CE_train, model.converge, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platt Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def platt_results(X_train, y_train, X_test, y_test):\n",
    "    #smoothed Labels\n",
    "    NP = (y_train > 0.5).sum()\n",
    "    NN = len(y_train) - NP\n",
    "    y_train_smoothed = np.zeros(len(y_train))\n",
    "    y_tmp = y_train.values\n",
    "    for i in range(len(y_train)):\n",
    "        if y_tmp[i] > 0:\n",
    "            y_train_smoothed[i] = 1 - 1 / (NP + 2)\n",
    "        else:\n",
    "            y_train_smoothed[i] = 1 / (NN + 2)\n",
    "        \n",
    "    model = LR(learnRate = LRate, nIter = nIter, smoothed = True)\n",
    "    model.SGD(X_train.values, y_train.values, X_test.values, y_test.values, y_train_smoothed)\n",
    "    \n",
    "    y_pred_test  = model.predict_prob(X_test)\n",
    "    y_pred_train = model.predict_prob(X_train)\n",
    "    \n",
    "    CE_test  = model.crossEntropy(y_pred_test,  y_test)\n",
    "    CE_train = model.crossEntropy(y_pred_train, y_train)\n",
    "    \n",
    "    y_pred_test_labels  = model.predict(X_test)\n",
    "    y_pred_train_labels = model.predict(X_train)\n",
    "    \n",
    "    acc_test  = accuracy_score(y_test,  y_pred_test_labels ) * 100\n",
    "    acc_train = accuracy_score(y_train, y_pred_train_labels) * 100\n",
    "    \n",
    "    return acc_test, CE_test, acc_train, CE_train, model.converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_results(X_train, y_train, X_test, y_test, model):\n",
    "    model_sklearn = LogisticRegression(random_state=24, solver='lbfgs').fit(X_train, y_train)\n",
    "    y_pred_test  = model_sklearn.predict_proba(X_test)\n",
    "    y_pred_train = model_sklearn.predict_proba(X_train)\n",
    "    \n",
    "    CE_test  = model.crossEntropy(y_pred_test[:,1] , y_test)\n",
    "    CE_train = model.crossEntropy(y_pred_train[:,1], y_train)\n",
    "    \n",
    "    y_pred_test_labels  = model_sklearn.predict(X_test)\n",
    "    y_pred_train_labels = model_sklearn.predict(X_train)\n",
    "    \n",
    "    acc_test  = accuracy_score(y_test,  y_pred_test_labels ) * 100\n",
    "    acc_train = accuracy_score(y_train, y_pred_train_labels) * 100\n",
    "    \n",
    "    return acc_test, CE_test, acc_train, CE_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Kernels of positive and negative classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernels(X_train, y_train):\n",
    "    #split training set into positive and negative instances (S1, S0)\n",
    "    poss = []\n",
    "    negs = []\n",
    "    y_tmp = y_train.values\n",
    "    for i in range(len(y_train)):\n",
    "        if y_tmp[i] == 0:\n",
    "            negs.append(i)\n",
    "        else:\n",
    "            poss.append(i)\n",
    "    S0 = (X_train.iloc[negs,:]).T.values\n",
    "    S1 = (X_train.iloc[poss,:]).T.values\n",
    "    #Create positive and negative classes kernels\n",
    "    neg_kernel = stats.gaussian_kde(S0)\n",
    "    pos_kernel = stats.gaussian_kde(S1)\n",
    "    \n",
    "    #Calculate smoothed labels based on values from the kernels\n",
    "    P_S0 = pos_kernel.evaluate(S0) / (pos_kernel.evaluate(S0) + neg_kernel.evaluate(S0))\n",
    "    P_S1 = pos_kernel.evaluate(S1) / (pos_kernel.evaluate(S1) + neg_kernel.evaluate(S1))\n",
    "    y_train_prob = np.zeros(len(y_train))\n",
    "    neg_index = 0\n",
    "    pos_index = 0\n",
    "    for i in range(len(y_train)):\n",
    "        if y_tmp[i] == 0:\n",
    "            y_train_prob[i] = P_S0[neg_index]\n",
    "            neg_index += 1\n",
    "        else:\n",
    "            y_train_prob[i] = P_S1[pos_index]\n",
    "            pos_index += 1\n",
    "            \n",
    "    return pos_kernel, neg_kernel, y_train_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M1_results(X_train, y_train, X_test, y_test, y_train_prob):\n",
    "    #Train the LR model with the smoothed labels\n",
    "    model = LR(learnRate = LRate, nIter = nIter, smoothed = True)\n",
    "    model.SGD(X_train.values, y_train.values, X_test.values, y_test.values, y_train_prob)\n",
    "    \n",
    "    y_pred_test  = model.predict_prob(X_test)\n",
    "    y_pred_train = model.predict_prob(X_train)\n",
    "    \n",
    "    CE_test  = model.crossEntropy(y_pred_test,  y_test)\n",
    "    CE_train = model.crossEntropy(y_pred_train, y_train)\n",
    "    \n",
    "    y_pred_test_labels  = model.predict(X_test)\n",
    "    y_pred_train_labels = model.predict(X_train)\n",
    "    \n",
    "    acc_test  = accuracy_score(y_test,  y_pred_test_labels ) * 100\n",
    "    acc_train = accuracy_score(y_train, y_pred_train_labels) * 100\n",
    "    \n",
    "    return acc_test, CE_test, acc_train, CE_train, model.converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M2_results(X_train, y_train, X_test, y_test, pos_kernel, neg_kernel):\n",
    "    N0_2 = int((y_train == 0).sum() * 2)\n",
    "    N1_2 = int((y_train == 1).sum() * 2)\n",
    "    neg2 = neg_kernel.resample(N0_2)\n",
    "    pos2 = pos_kernel.resample(N1_2)\n",
    "\n",
    "    X_train2 = np.array(np.concatenate((neg2.T, pos2.T), axis=0) )\n",
    "    y_train2 = np.asarray([0] * N0_2 + [1] * N1_2)\n",
    "    \n",
    "    #Train the LR model with Method2\n",
    "    model = LR(learnRate = LRate, nIter = nIter)\n",
    "    model.SGD(X_train2, y_train2, X_test, y_test)\n",
    "    \n",
    "    y_pred_test  = model.predict_prob(X_test)\n",
    "    y_pred_train = model.predict_prob(X_train)\n",
    "    \n",
    "    CE_test  = model.crossEntropy(y_pred_test,  y_test)\n",
    "    CE_train = model.crossEntropy(y_pred_train, y_train)\n",
    "    \n",
    "    y_pred_test_labels  = model.predict(X_test)\n",
    "    y_pred_train_labels = model.predict(X_train)\n",
    "    \n",
    "    acc_test  = accuracy_score(y_test,  y_pred_test_labels ) * 100\n",
    "    acc_train = accuracy_score(y_train, y_pred_train_labels) * 100\n",
    "    \n",
    "    return acc_test, CE_test, acc_train, CE_train, model.converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Name: Datasets\\aecoli.csv\n",
      "Start Normal:\n",
      "Finished  0  iterations --> Test CE: 1\n",
      "100\n",
      "Start Platt:\n",
      "Finished  0  iterations --> Test CE: 1\n",
      "Start SKLearn:\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-faa4fe75d4c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m#KDE Kernels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mpos_kernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_kernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;31m#Method 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Start Method1:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-86a8eafafe3a>\u001b[0m in \u001b[0;36mkernels\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#Create positive and negative classes kernels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mneg_kernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgaussian_kde\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mpos_kernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgaussian_kde\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#Calculate smoothed labels based on values from the kernels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\stats\\kde.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, bw_method)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_bandwidth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbw_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\stats\\kde.py\u001b[0m in \u001b[0;36mset_bandwidth\u001b[1;34m(self, bw_method)\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_covariance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_compute_covariance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\stats\\kde.py\u001b[0m in \u001b[0;36m_compute_covariance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    508\u001b[0m             self._data_covariance = atleast_2d(np.cov(self.dataset, rowvar=1,\n\u001b[0;32m    509\u001b[0m                                                bias=False))\n\u001b[1;32m--> 510\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_inv_cov\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_covariance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcovariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_covariance\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a, overwrite_a, check_finite)\u001b[0m\n\u001b[0;32m    973\u001b[0m         \u001b[0minv_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlwork\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite_lu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    976\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m         raise ValueError('illegal value in %d-th argument of internal '\n",
      "\u001b[1;31mLinAlgError\u001b[0m: singular matrix"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    # Read Dataset\n",
    "    print('Dataset Name:', f)\n",
    "    data = pd.read_csv(f)\n",
    "    rows, cols, ratio = meta_features(data)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop('class', axis=1), data['class'], \n",
    "                                                        test_size=0.2, random_state=24)\n",
    "    X_train = (X_train - X_train.mean()) / X_train.std()\n",
    "    X_test = (X_test - X_train.mean()) / X_train.std()\n",
    "    res['Name'] = f; res['Instances'] = rows; res['Features'] = cols; res['PosClassRatio'] = ratio;\n",
    "   \n",
    "    #Normal LR\n",
    "    print('Start Normal:')\n",
    "    ac1, ce1, ac2, ce2, converge, model = normal_results(X_train, y_train, X_test, y_test)\n",
    "    res['NTestCE'] = ce1; res['NTestAcc'] = ac1; res['NTrainCE'] = ce2; res['NTrainAcc'] = ac2; res['NConverge'] = converge\n",
    "    \n",
    "    #Platt LR\n",
    "    print('Start Platt:')\n",
    "    ac1, ce1, ac2, ce2, converge = platt_results(X_train, y_train, X_test, y_test)\n",
    "    res['PTestCE'] = ce1; res['PTestAcc'] = ac1; res['PTrainCE'] = ce2; res['PTrainAcc'] = ac2; res['PConverge'] = converge\n",
    "    \n",
    "    #Sklearn Results\n",
    "    print('Start SKLearn:')\n",
    "    ac1, ce1, ac2, ce2 = sklearn_results(X_train, y_train, X_test, y_test, model)\n",
    "    res['SKTestCE'] = ce1; res['SKTestAcc'] = ac1; res['SKTrainCE'] = ce2; res['SKTrainAcc'] = ac2;\n",
    "     \n",
    "    #KDE Kernels\n",
    "    pos_kernel, neg_kernel, y_train_prob = kernels(X_train, y_train)\n",
    "    #Method 1\n",
    "    print('Start Method1:')\n",
    "    ac1, ce1, ac2, ce2, converge = M1_results(X_train, y_train, X_test, y_test, y_train_prob)\n",
    "    res['M1TestCE'] = ce1; res['M1TestAcc'] = ac1; res['M1TrainCE'] = ce2; res['M1TrainAcc'] = ac2; res['M1Converge'] = converge\n",
    "    #Method 2\n",
    "    print('Start Method2:')\n",
    "    ac1, ce1, ac2, ce2, converge = M2_results(X_train, y_train, X_test, y_test, pos_kernel, neg_kernel)\n",
    "    res['M2TestCE'] = ce1; res['M2TestAcc'] = ac1; res['M2TrainCE'] = ce2; res['M2TrainAcc'] = ac2; res['M2Converge'] = converge\n",
    "    \n",
    "    results = results.append(res, ignore_index=True)\n",
    "    print(res, '\\n########################################\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('Evaluation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
